{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "spacy.load('en')\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \n",
    "SYMBOLCHARS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OffensiveLangDF = pd.read_csv('../Data/Offensive Language Dataset/Cleaned_labeled_data.csv')\n",
    "spamSmsDF = pd.read_csv('../Data/SMS Spam Dataset/Cleaned_SMSSpamCollection.csv')\n",
    "politicalDF = pd.read_csv('../Data/Indian Political Tweets Dataset/cleaned-tweets.csv')\n",
    "\n",
    "currentDF = politicalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(textData):\n",
    "\n",
    "    textData = textData.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    textData = textData.lower()\n",
    "    tokens = parser(textData)\n",
    "\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    tokens = [tok for tok in tokens if tok.lower() not in STOPWORDS]\n",
    "    # Remove Symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLCHARS]\n",
    "    # Remove words with less than 3 characters\n",
    "    tokens = [tok for tok in tokens if len(tok) >= 3]\n",
    "    # Remove Non-Alphabetic Characters\n",
    "    tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "    \n",
    "    # Stemming of Words\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    tokens = list(set(tokens))\n",
    "    textData = ' '.join(tokens[:])\n",
    "    return textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hateSpeechDF['text'] = hateSpeechDF['text'].apply(lambda x:tokenizeText(x))\n",
    "currentDF['text'] = currentDF['text'].apply(lambda x:tokenizeText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179                                                     \n",
      "3857    token complet gift india nehru peac minist kab...\n",
      "1241    littl googl mayb work understand voic english ...\n",
      "4080    congress read thought age save india need narr...\n",
      "2143    fan india guy hyderabad superb everyday trend ...\n",
      "                              ...                        \n",
      "561                 tell movi wanna think tag hello today\n",
      "393                                                      \n",
      "2192            landmark relat daniel visit import carmon\n",
      "4536    safe akhilesh india samajwadi border yadav pol...\n",
      "1190         insur trade organ free critic membership ill\n",
      "Name: text, Length: 4545, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(currentDF['text'], currentDF['category'])\n",
    "print (train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(currentDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 10204)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(currentDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(currentDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(currentDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.classification_report(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Classification Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes | Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.84      0.97      0.90       453\n",
      "         POL       0.99      0.92      0.95      1062\n",
      "\n",
      "    accuracy                           0.94      1515\n",
      "   macro avg       0.92      0.95      0.93      1515\n",
      "weighted avg       0.94      0.94      0.94      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes | Word level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.70      0.98      0.82       371\n",
      "         POL       0.99      0.86      0.92      1144\n",
      "\n",
      "    accuracy                           0.89      1515\n",
      "   macro avg       0.85      0.92      0.87      1515\n",
      "weighted avg       0.92      0.89      0.90      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes | N-Gram level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.17      0.64      0.26       137\n",
      "         POL       0.95      0.69      0.80      1378\n",
      "\n",
      "    accuracy                           0.68      1515\n",
      "   macro avg       0.56      0.66      0.53      1515\n",
      "weighted avg       0.88      0.68      0.75      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes | Character level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.69      0.98      0.81       364\n",
      "         POL       0.99      0.86      0.92      1151\n",
      "\n",
      "    accuracy                           0.89      1515\n",
      "   macro avg       0.84      0.92      0.86      1515\n",
      "weighted avg       0.92      0.89      0.89      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
