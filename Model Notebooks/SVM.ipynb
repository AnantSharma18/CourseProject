{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "spacy.load('en')\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) \n",
    "SYMBOLCHARS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OffensiveLangDF = pd.read_csv('../Data/Offensive Language Dataset/Cleaned_labeled_data.csv')\n",
    "spamSmsDF = pd.read_csv('../Data/SMS Spam Dataset/Cleaned_SMSSpamCollection.csv')\n",
    "politicalDF = pd.read_csv('../Data/Indian Political Tweets Dataset/cleaned-tweets.csv')\n",
    "\n",
    "currentDF = politicalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(textData):\n",
    "\n",
    "    textData = textData.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    textData = textData.lower()\n",
    "    tokens = parser(textData)\n",
    "\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    tokens = [tok for tok in tokens if tok.lower() not in STOPWORDS]\n",
    "    # Remove Symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLCHARS]\n",
    "    # Remove words with less than 3 characters\n",
    "    tokens = [tok for tok in tokens if len(tok) >= 3]\n",
    "    # Remove Non-Alphabetic Characters\n",
    "    tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "    \n",
    "    # Stemming of Words\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    tokens = list(set(tokens))\n",
    "    textData = ' '.join(tokens[:])\n",
    "    return textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hateSpeechDF['text'] = hateSpeechDF['text'].apply(lambda x:tokenizeText(x))\n",
    "currentDF['text'] = currentDF['text'].apply(lambda x:tokenizeText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5697      like queen treat india view nation lang special\n",
      "2717    lost pakistan caus ghirti hui india support di...\n",
      "3119                                          time go tdp\n",
      "2067    defeat insiderupa polit minu elect india relig...\n",
      "1857        work anybodi fake want diddi let reason think\n",
      "                              ...                        \n",
      "1285                     select costum final angel fallen\n",
      "4286    effect bihar chief bureaucraci india rejig nit...\n",
      "1181                     love great rate kid right gorgou\n",
      "2442    cemetari visit haifa narendramodi live india i...\n",
      "5815    prime rememb india leader shri rajivgandhi vis...\n",
      "Name: text, Length: 4545, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(currentDF['text'], currentDF['category'])\n",
    "print (train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(currentDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 10204)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(currentDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(currentDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(currentDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.classification_report(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Classification Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM | Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.95      0.88      0.92       568\n",
      "         POL       0.93      0.97      0.95       947\n",
      "\n",
      "    accuracy                           0.94      1515\n",
      "   macro avg       0.94      0.93      0.93      1515\n",
      "weighted avg       0.94      0.94      0.94      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(SVC(kernel='linear',decision_function_shape='ovo'), xtrain_count, train_y, xvalid_count)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM | Word level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.98      0.88      0.92       584\n",
      "         POL       0.93      0.99      0.96       931\n",
      "\n",
      "    accuracy                           0.94      1515\n",
      "   macro avg       0.95      0.93      0.94      1515\n",
      "weighted avg       0.95      0.94      0.94      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(SVC(kernel='linear',decision_function_shape='ovo'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM | N-Gram level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.14      0.92      0.25        83\n",
      "         POL       0.99      0.69      0.81      1432\n",
      "\n",
      "    accuracy                           0.70      1515\n",
      "   macro avg       0.57      0.80      0.53      1515\n",
      "weighted avg       0.95      0.70      0.78      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(SVC(kernel='linear',decision_function_shape='ovo'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM | Character level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NOTPOL       0.95      0.86      0.90       580\n",
      "         POL       0.92      0.97      0.94       935\n",
      "\n",
      "    accuracy                           0.93      1515\n",
      "   macro avg       0.93      0.92      0.92      1515\n",
      "weighted avg       0.93      0.93      0.93      1515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(SVC(kernel='linear',decision_function_shape='ovo'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
